{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                  \n",
    "import numpy as np         \n",
    "import tensorflow as tf    \n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-7. 프로젝트: 멋진 작사가 만들기\n",
    "\n",
    "- [[참고]](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/)\n",
    "\n",
    "- [[Text Generation with miniature GPT]](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 읽어오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['The Cat in the Hat', 'By Dr. Seuss', 'The sun did not shine.', 'It was too wet to play.', 'So we sat in the house', 'All that cold cold wet day.', 'I sat there with Sally.', 'We sat there we two.', 'And I said How I wish']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel//lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정제\n",
    "\n",
    "---\n",
    "\n",
    "- 추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 문장을 토큰화 했을 때 **토큰의 개수가 15개**를 넘어가면 잘라내기를 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cat in the Hat\n",
      "By Dr. Seuss\n",
      "The sun did not shine.\n",
      "It was too wet to play.\n",
      "So we sat in the house\n",
      "All that cold cold wet day.\n",
      "I sat there with Sally.\n",
      "We sat there we two.\n",
      "And I said How I wish\n",
      "We had something to do!\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 \n",
    "- 문장을 일정한 기준으로 쪼개기 (띄어쓰기를 기준으로) \n",
    "- 문장 부호 양쪽에 공백 추가 , 소문자로 변환 , 특수문자  제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       \n",
    "    # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        \n",
    "    # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  \n",
    "    # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  \n",
    "    # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      \n",
    "    # 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소스 문장(Source Sentence) = X_train\n",
    "#### 타겟 문장(Target Sentence) = y_train\n",
    "- 토큰화를 진행한 후 끝 단어 를 없애면 소스 문장, 첫 단어 를 없애면 타겟 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> the cat in the hat <end>',\n",
       " '<start> by dr . seuss <end>',\n",
       " '<start> the sun did not shine . <end>',\n",
       " '<start> it was too wet to play . <end>',\n",
       " '<start> so we sat in the house <end>',\n",
       " '<start> all that cold cold wet day . <end>',\n",
       " '<start> i sat there with sally . <end>',\n",
       " '<start> we sat there we two . <end>',\n",
       " '<start> and i said how i wish <end>',\n",
       " '<start> we had something to do ! <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175749"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 데이터셋 분리\n",
    "---\n",
    "- `tokenize()` 함수로 데이터를 Tensor로 변환한 후, \n",
    "- sklearn 모듈의 `train_test_split()` 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. \n",
    "- **단어장의 크기는 12,000 이상**으로 설정하세요! \n",
    "- 총 **데이터의 20% 를 평가 데이터셋**으로 사용해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    6  903 ...    0    0    0]\n",
      " [   2  122 2584 ...    0    0    0]\n",
      " [   2    6  304 ...    0    0    0]\n",
      " ...\n",
      " [ 673   27    6 ...    6  189    3]\n",
      " [   2  673   27 ...    0    0    0]\n",
      " [   2  673   27 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f447269e7d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어 개수 \n",
    "        filters=' ',    \n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) \n",
    "    # corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    # padding 뒤에 , 토큰 개수 15개\n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `num_words`: 단어 빈도에 따른 사용할 단어 개수의 최대값. 가장 빈번하게 사용되는 num_words개의 단어만 보존합니다.\n",
    "- `filters`: 문자열로, 각 성분이 텍스트에서 걸러진 문자에 해당됩니다. 디폴트 값은 모든 구두점이며, 거기에 탭과 줄 바꿈은 추가하고 ' 문자는 제외합니다.\n",
    "- `lower`: 불리언. 텍스트를 소문자로 변환할지 여부.\n",
    "- `split`: 문자열. 단어 분해 용도의 분리기.\n",
    "- `char_level`: 참인 경우 모든 문자가 토큰으로 처리됩니다.\n",
    "- `oov_token`: 값이 지정된 경우, text_to_sequence 호출 과정에서 단어색인(word_index)에 추가되어 어휘목록 외 단어를 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    6  903   14    6 1350    3    0    0    0]\n",
      " [   2  122 2584   20    1    3    0    0    0    0]\n",
      " [   2    6  304  166   70  559   20    3    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tartget / Source 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    6  903   14    6 1350    3    0    0    0    0    0    0    0]\n",
      "[   6  903   14    6 1350    3    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. \n",
    "# 마지막 토큰은 <END>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "# tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공지능 만들기\n",
    "\n",
    "- 모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! \n",
    "- (Loss는 아래 제시된 Loss 함수를 그대로 사용!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.drop_1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.drop_1(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "# embedding_size = 256\n",
    "# hidden_size = 1024\n",
    "\n",
    "embedding_size = 1020\n",
    "hidden_size = 2048\n",
    "\n",
    "/\n",
    "vocab_size = tokenizer.num_words + 1\n",
    "\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  `vocab_size` : Embedding 레이어에 어휘 크기를 지정할 때 실제 어휘보다 1 더 크게 지정합니다. (12001)\n",
    "\n",
    "- `embedding_size` : 워드 벡터의 차원수\n",
    "\n",
    "- `hidden_size` : LSTM 레이어의 hidden state 의 차원수\n",
    "\n",
    "임베딩 레이어의 크기는 각 단어가 나타내는 벡터의 길이입니다. \n",
    "이것은 일반적으로 100-500 사이의 영역에 있습니다. \n",
    "즉, 임베딩 레이어 크기가 250 인 경우 각 단어는 250- 길이 벡터로 표현됩니다.\n",
    "\n",
    "#### LSTM 히든 레이어 크기\n",
    "- 일반적으로 임베딩 레이어 출력의 크기를 LSTM 셀의 히든 레이어 수와 일치시킵니다. \n",
    "- LSTM 셀의 숨겨진 계층이 어디에서 왔는지 궁금 할 것입니다. \n",
    "- 셀의 각  sigmoid,  tanh 또는  은닉 상태 레이어는 실제로 노드의 집합이며, 그 수는 은닉 레이어  크기와 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4394/4394 [==============================] - 572s 130ms/step - loss: 3.1079 - val_loss: 2.7861\n",
      "Epoch 2/10\n",
      "4394/4394 [==============================] - 587s 134ms/step - loss: 2.5833 - val_loss: 2.5265\n",
      "Epoch 3/10\n",
      "4394/4394 [==============================] - 587s 134ms/step - loss: 2.1988 - val_loss: 2.3824\n",
      "Epoch 4/10\n",
      "4394/4394 [==============================] - 589s 134ms/step - loss: 1.8963 - val_loss: 2.3078\n",
      "Epoch 5/10\n",
      "4394/4394 [==============================] - 575s 131ms/step - loss: 1.6782 - val_loss: 2.2833\n",
      "Epoch 6/10\n",
      "4394/4394 [==============================] - 531s 121ms/step - loss: 1.5270 - val_loss: 2.2936\n",
      "Epoch 7/10\n",
      "4394/4394 [==============================] - 567s 129ms/step - loss: 1.4271 - val_loss: 2.3041\n",
      "Epoch 8/10\n",
      "4394/4394 [==============================] - 585s 133ms/step - loss: 1.3611 - val_loss: 2.3332\n",
      "Epoch 9/10\n",
      "4394/4394 [==============================] - 544s 124ms/step - loss: 1.3188 - val_loss: 2.3527\n",
      "Epoch 10/10\n",
      "4394/4394 [==============================] - 527s 120ms/step - loss: 1.2911 - val_loss: 2.3742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f44708c92d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   \n",
    "        # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start>  i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  10 Epoch에서 val_loss 값을 2.2 목표 \n",
    "\n",
    "- Embedding size 와 hidden size를 늘려가면서 학습시키기 \n",
    "\n",
    "\n",
    "#### 1. val_loss: 2.6070\n",
    "\n",
    "```python \n",
    "embedding_size = 600\n",
    "hidden_size = 3000\n",
    "```\n",
    "- '<start> i love you , i m not gonna crack <end> '\n",
    "\n",
    "\n",
    "\n",
    "#### 2. ResourceExhaustedError\n",
    "\n",
    "\n",
    "```python\n",
    "embedding_size = 1500\n",
    "hidden_size = 5000\n",
    "```\n",
    "- 학습 실패 :  ResourceExhaustedError: OOM when allocating tensor with shape[20000,5000]\n",
    "\n",
    "\n",
    "#### 3. val_loss: 2.6012\n",
    "\n",
    "\n",
    "```python\n",
    "embedding_size = 600\n",
    "hidden_size = 2000\n",
    "```\n",
    "- '<start> i love it when you call me big poppa <end> '\n",
    "    \n",
    "#### 4. val_loss: 2.5906\n",
    "\n",
    "\n",
    "```python\n",
    "embedding_size = 600\n",
    "hidden_size = 1500\n",
    "```\n",
    "- '<star t> i love you mom , you always be my favorite girl . <end> '\n",
    "   \n",
    "     \n",
    "#### 5. ResourceExhaustedError\n",
    "\n",
    "\n",
    "```python\n",
    "embedding_size = 1200\n",
    "hidden_size = 2400\n",
    "```\n",
    "- 학습 실패 :  ResourceExhaustedError: OOM when allocating tensor with shape[20000,5000]\n",
    "\n",
    "    \n",
    "#### 6. loss: 1.0426 - val_loss: 2.5855\n",
    "\n",
    "\n",
    "```python\n",
    "embedding_size = 726\n",
    "hidden_size = 1800\n",
    "```\n",
    "- '<start> i love you , liberian girl <end> '\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- embedding과 hidden size만 조정해서는 5 epoch 이후에는  다시 validation loss값이 늘어난다. \n",
    "- drop out layer를 추가해서 과적합 해결 시도 \n",
    "- drop out (0.2) - epoch 6에서 다시 loss값 증가\n",
    "```python \n",
    "Epoch 5/10\n",
    "4394/4394 [==============================] - 443s 101ms/step - loss: 1.5711 - val_loss: 2.2955\n",
    "Epoch 6/10\n",
    "4394/4394 [==============================] - 443s 101ms/step - loss: 1.4105 - val_loss: 2.3132\n",
    "```\n",
    "\n",
    "- drop out (0.3) - epoch 6에서 다시 loss값 증가\n",
    "- 10 epoch 에서 loss: 1.2750 - val_loss: 2.3837\n",
    "```python\n",
    "Epoch 5/10\n",
    "4394/4394 [==============================] - 440s 100ms/step - loss: 1.6930 - val_loss: 2.2945\n",
    "Epoch 6/10\n",
    "4394/4394 [==============================] - 440s 100ms/step - loss: 1.5342 - val_loss: 2.2952\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
