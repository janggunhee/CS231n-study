{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 폐렴을 진단해 보자\n",
    "\n",
    "#### 데이터셋\n",
    "\n",
    "이번 노드에서 사용할 데이터는 **캐글의 Chest X-Ray Images** 입니다.\n",
    "폴더를 생성하고 캐글에서 데이터를 다운받아주세요.\n",
    "\n",
    "[Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "위와 같이 압축을 풀고 나면, 데이터의 구성은 ~/aiffel/chest_xray 하위에 3개의 폴더 (train, test, val)로 구성되며, 각 이미지 카테고리 (폐렴 / 정상)에 대한 하위 폴더를 포함합니다. 5,856 개의 X-Ray 이미지 (JPEG)와 2 개의 범주 (폐렴 / 정상)가 있습니다. 전체 데이터의 크기는 총 1.2GB 가량 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    # 정규표현식 관련된 작업에 필요한 패키지\n",
    "import os    # I/O 관련된 작업에 필요한 패키지 \n",
    "import pandas as pd     # 데이터 전처리 관련된 작업에 필요한 패키지\n",
    "import numpy as np      # 데이터 array 작업에 필요한 패키지\n",
    "import tensorflow as tf  # 딥러닝 관련된 작업에 필요한 패키지\n",
    "import matplotlib.pyplot as plt    # 데이터 시각화에 관련된 작업에 필요한 패키지\n",
    "from sklearn.model_selection import train_test_split  # 데이터 전처리에 필요한 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/aiffel\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드할 때 빠르게 로드할 수 있도록하는 설정 변수\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# 데이터 ROOT 경로 변수\n",
    "ROOT_PATH = os.path.join(os.getenv('HOME'), 'aiffel')\n",
    "\n",
    "# BATCH_SIZE 변수\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# X-RAY 이미지 사이즈 변수\n",
    "IMAGE_SIZE = [180, 180]\n",
    "\n",
    "# EPOCH 크기 변수\n",
    "EPOCHS = 25\n",
    "\n",
    "print(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216\n",
      "624\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "train_filenames = tf.io.gfile.glob(str(ROOT_PATH + '/chest_xray/train/*/*'))\n",
    "test_filenames = tf.io.gfile.glob(str(ROOT_PATH + '/chest_xray/test/*/*'))\n",
    "val_filenames = tf.io.gfile.glob(str(ROOT_PATH + '/chest_xray/val/*/*'))\n",
    "\n",
    "print(len(train_filenames))\n",
    "print(len(test_filenames))\n",
    "print(len(val_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train 안에는 5216개, test 안에는 624개, val 안에는 16개가 있습니다.\n",
    "\n",
    "- 갯수 비는 89%, 10.7%, 0.3% 입니다. val 갯수가 너무 없기 때문에 train에서 val에 쓰일 데이터를 더 가져와보겠습니다.\n",
    "\n",
    "- train과 val에 있는 데이터를 모은 다음에 train : val를 80:20으로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4185\n",
      "1047\n"
     ]
    }
   ],
   "source": [
    "filenames = tf.io.gfile.glob(str(ROOT_PATH + '/chest_xray/train/*/*'))\n",
    "filenames.extend(tf.io.gfile.glob(str(ROOT_PATH + '/chest_xray/val/*/*')))\n",
    "\n",
    "# train, test(val) dataset으로 분할. test_size에 0.2는 20%롤 의미함.\n",
    "train_filenames, val_filenames = train_test_split(filenames, test_size=0.2)\n",
    "\n",
    "print(len(train_filenames))\n",
    "print(len(val_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train은 4185개, test는 624개, val은 1047개가 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  정상이미지 수/ 폐렴이미지 수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal images count in training set: 1102\n",
      "Pneumonia images count in training set: 3083\n"
     ]
    }
   ],
   "source": [
    "COUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\n",
    "print(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n",
    "\n",
    "COUNT_PNEUMONIA = len([filename for filename in train_filenames if \"PNEUMONIA\" in filename])\n",
    "print(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과를 보면 정상보다 폐렴 이미지 수가 3배 더 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\n",
    "val_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images count: 4185\n",
      "Validating images count: 1047\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\n",
    "print(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n",
    "\n",
    "VAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\n",
    "print(\"Validating images count: \" + str(VAL_IMG_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
